# -*- coding: utf-8 -*-
"""
Created on Fri Dec  6 18:27:11 2024

@author: 1
"""

import torch
import jieba
import seaborn
import numpy as np
import torch.nn as nn
import math, copy, time
import torch.nn.functional as F

from tqdm import tqdm
from torchtext import data
from torch.nn import LayerNorm
import matplotlib.pyplot as plt
from torchtext.data import Field
from torch.autograd import Variable
from nltk.translate.bleu_score import sentence_bleu

class EncoderDecoder(nn.Module):
    """
    A standard Encoder-Decoder architecture. Base for this and many 
    other models.
    """
    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):
        super(EncoderDecoder, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.src_embed = src_embed
        self.tgt_embed = tgt_embed
        self.generator = generator
        
    def forward(self, src, tgt, src_mask, tgt_mask):
        "Take in and process masked src and target sequences."
        return self.decode(self.encode(src, src_mask), src_mask,
                            tgt, tgt_mask)
    
    def encode(self, src, src_mask):
        return self.encoder(self.src_embed(src), src_mask)
    
    def decode(self, memory, src_mask, tgt, tgt_mask):
        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)
    
class Generator(nn.Module):
    "Define standard linear + softmax generation step."
    def __init__(self, d_model, vocab):
        super(Generator, self).__init__()
        self.proj = nn.Linear(d_model, vocab)

    def forward(self, x):
        return F.log_softmax(self.proj(x), dim=-1)

def clones(module, N):
    "Produce N identical layers."
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])

class Encoder(nn.Module):
    "Core encoder is a stack of N layers"
    def __init__(self, layer, N):
        super(Encoder, self).__init__()
        self.layers = clones(layer, N)
        self.norm = LayerNorm(layer.size)
        
    def forward(self, x, mask):
        "Pass the input (and mask) through each layer in turn."
        for layer in self.layers:
            x = layer(x, mask)
        return self.norm(x)
'''
a = torch.rand((6,4))
mean = a.mean(-1,keepdim=True)
std = a.std(-1,keepdim=True)
a - mean
a.mean(-1)
''' 
# 标准化层，除了对最后一列做标准化操作以外，还在矩阵层面做了相乘和相加，size就是矩阵层面的形状 
# 感觉shape两个维度必须相等 
# size是d_model可以自动扩展，所以只要最后一个维度正确即可
class LayerNorm(nn.Module):
    "Construct a layernorm module (See citation for details)."
    def __init__(self, features, eps=1e-6):
        super(LayerNorm, self).__init__()
        self.a_2 = nn.Parameter(torch.ones(features))
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2
# 实现先标准化然后再执行某个函数    
class SublayerConnection(nn.Module):
    """
    A residual connection followed by a layer norm.
    Note for code simplicity the norm is first as opposed to last.
    """
    def __init__(self, size, dropout):
        super(SublayerConnection, self).__init__()
        self.norm = LayerNorm(size)
        self.dropout = nn.Dropout(dropout)
    # 已自注意力机制，已经输入部分参数之后的降元函数作为输入
    def forward(self, x, sublayer):
        "Apply residual connection to any sublayer with the same size."
        return x + self.dropout(sublayer(self.norm(x)))
# self_attn多头注意力机制，feed_forward维度不变的变换，增强模型泛化能力   
class EncoderLayer(nn.Module):
    "Encoder is made up of self-attn and feed forward (defined below)"
    def __init__(self, size, self_attn, feed_forward, dropout):
        super(EncoderLayer, self).__init__()
        self.self_attn = self_attn
        self.feed_forward = feed_forward
        self.sublayer = clones(SublayerConnection(size, dropout), 2)
        self.size = size

    def forward(self, x, mask):
        "Follow Figure 1 (left) for connections."
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
        return self.sublayer[1](x, self.feed_forward)
class Decoder(nn.Module):
    "Generic N layer decoder with masking."
    def __init__(self, layer, N):
        super(Decoder, self).__init__()
        self.layers = clones(layer, N)
        self.norm = LayerNorm(layer.size)
        
    def forward(self, x, memory, src_mask, tgt_mask):
        for layer in self.layers:
            x = layer(x, memory, src_mask, tgt_mask)
        return self.norm(x)
    
class DecoderLayer(nn.Module):
    "Decoder is made of self-attn, src-attn, and feed forward (defined below)"
    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):
        super(DecoderLayer, self).__init__()
        self.size = size
        self.self_attn = self_attn
        self.src_attn = src_attn
        self.feed_forward = feed_forward
        self.sublayer = clones(SublayerConnection(size, dropout), 3)
 
    def forward(self, x, memory, src_mask, tgt_mask):
        "Follow Figure 1 (right) for connections."
        m = memory
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))
        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))
        return self.sublayer[2](x, self.feed_forward)
'''
size = 10
'''    
def subsequent_mask(size):
    "Mask out subsequent positions."
    attn_shape = (1, size, size)
    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')
    return torch.from_numpy(subsequent_mask) == 0

def attention(query, key, value, mask=None, dropout=None):
    "Compute 'Scaled Dot Product Attention'"
    d_k = query.size(-1)
    scores = torch.matmul(query, key.transpose(-2, -1)) \
             / math.sqrt(d_k)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    p_attn = F.softmax(scores, dim = -1)
    if dropout is not None:
        p_attn = dropout(p_attn)
    return torch.matmul(p_attn, value), p_attn
'''
d_k = d_model // h
h = h
linears = clones(nn.Linear(d_model, d_model), 4)
attn = None
dropout = nn.Dropout(p=dropout)
batch_size = 32
sequence_len = 10
x = torch.rand((batch_size,sequence_len,d_model))
x.shape
attn_shape = (1, sequence_len, sequence_len)
subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')
subsequent_mask.shape
mask = torch.from_numpy(subsequent_mask)
mask.shape
nbatches = query.size(0)
query,key,value = x,x,x
query, key, value = \
    [l(x).view(nbatches, -1, h, d_k).transpose(1, 2)
     for l, x in zip(linears, (query, key, value))]
query.shape
key.shape
for l, x in zip(linears, [query, key, value]):
    print('d')
x, attn = attention(query, key, value, mask=mask, 
                         dropout=dropout)
scores.shape
scores[2,3,:,:]
mask.shape
p_attn.shape
p_attn[2,3,1,:]
attn.shape
attn[2,3,1,:]
x = x.transpose(1, 2).contiguous() \
     .view(nbatches, -1, h * self.d_k)
'''
class MultiHeadedAttention(nn.Module):
    def __init__(self, h, d_model, dropout=0.1):
        "Take in model size and number of heads."
        super(MultiHeadedAttention, self).__init__()
        assert d_model % h == 0
        # We assume d_v always equals d_k
        self.d_k = d_model // h
        self.h = h
        self.linears = clones(nn.Linear(d_model, d_model), 4)
        self.attn = None
        self.dropout = nn.Dropout(p=dropout)
        
    def forward(self, query, key, value, mask=None):
        "Implements Figure 2"
        if mask is not None:
            # Same mask applied to all h heads.
            mask = mask.unsqueeze(1)
        nbatches = query.size(0)
        # zip按照列表来确定循环次数的
        # 1) Do all the linear projections in batch from d_model => h x d_k 
        query, key, value = \
            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
             for l, x in zip(self.linears, (query, key, value))]
        
        # 2) Apply attention on all the projected vectors in batch. 
        x, self.attn = attention(query, key, value, mask=mask, 
                                 dropout=self.dropout)
        
        # 3) "Concat" using a view and apply a final linear. 
        x = x.transpose(1, 2).contiguous() \
             .view(nbatches, -1, self.h * self.d_k)
        return self.linears[-1](x)
# d_ff 是一个中间变量，最终变换的维度是不变的，变过去又变回来了    
class PositionwiseFeedForward(nn.Module):
    "Implements FFN equation."
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.w_2(self.dropout(F.relu(self.w_1(x))))

class Embeddings(nn.Module):
    def __init__(self, d_model, vocab):
        super(Embeddings, self).__init__()
        self.lut = nn.Embedding(vocab, d_model)
        self.d_model = d_model

    def forward(self, x):
        return self.lut(x) * math.sqrt(self.d_model)
'''
max_len = 1000
d_model = 512
pe.shape
position.shape
div_term.shape
q = position * div_term
q.shape
pe.shape
pe[:, :x.size(1),:].shape
'''
class PositionalEncoding(nn.Module):
    "Implement the PE function."
    def __init__(self, d_model, dropout, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        
        # Compute the positional encodings once in log space.
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) *
                             -(math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
        
    def forward(self, x):
        x = x + Variable(self.pe[:, :x.size(1)], 
                         requires_grad=False)
        return self.dropout(x)
    
def make_model(src_vocab, tgt_vocab, N=6, 
               d_model=512, d_ff=2048, h=8, dropout=0.1):
    "Helper: Construct a model from hyperparameters."
    c = copy.deepcopy
    attn = MultiHeadedAttention(h, d_model)
    ff = PositionwiseFeedForward(d_model, d_ff, dropout)
    position = PositionalEncoding(d_model, dropout)
    model = EncoderDecoder(
        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),
        Decoder(DecoderLayer(d_model, c(attn), c(attn), 
                             c(ff), dropout), N),
        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),
        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),
        Generator(d_model, tgt_vocab))
    
    # This was important from their code. 
    # Initialize parameters with Glorot / fan_avg.
    for p in model.parameters():
        if p.dim() > 1:
            nn.init.xavier_uniform(p)
    return model
'''
h = 8
d_model = 512
d_ff = 2048
dropout = 0.1
N = 6
src_vocab = 2000
tgt_vocab = 2000
'''
class Batch:
    "Object for holding a batch of data with mask during training."
    def __init__(self, src, trg=None, pad=0):
        self.src = src
        self.src_mask = (src != pad).unsqueeze(-2)
        if trg is not None:
            self.trg = trg[:, :-1]
            self.trg_y = trg[:, 1:]
            self.trg_mask = \
                self.make_std_mask(self.trg, pad)
            self.ntokens = (self.trg_y != pad).data.sum()
    
    @staticmethod
    def make_std_mask(tgt, pad):
        "Create a mask to hide padding and future words."
        tgt_mask = (tgt != pad).unsqueeze(-2)
        tgt_mask = tgt_mask & Variable(
            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))
        return tgt_mask

def run_epoch(data_iter, model, loss_compute):
    "Standard Training and Logging Function"
    start = time.time()
    total_tokens = 0
    total_loss = 0
    tokens = 0
    for i, batch in enumerate(data_iter):
        out = model.forward(batch.src, batch.trg, 
                            batch.src_mask, batch.trg_mask)
        loss = loss_compute(out, batch.trg_y, batch.ntokens)
        total_loss += loss
        total_tokens += batch.ntokens
        tokens += batch.ntokens
        if i % 50 == 1:
            elapsed = time.time() - start
            print("Epoch Step: %d Loss: %f Tokens per Sec: %f" %
                    (i, loss / batch.ntokens, tokens / elapsed))
            start = time.time()
            tokens = 0
    return total_loss / total_tokens

global max_src_in_batch, max_tgt_in_batch
def batch_size_fn(new, count, sofar):
    "Keep augmenting batch and calculate total number of tokens + padding."
    global max_src_in_batch, max_tgt_in_batch
    if count == 1:
        max_src_in_batch = 0
        max_tgt_in_batch = 0
    max_src_in_batch = max(max_src_in_batch,  len(new.src))
    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)
    src_elements = count * max_src_in_batch
    tgt_elements = count * max_tgt_in_batch
    return max(src_elements, tgt_elements)

class NoamOpt:
    "Optim wrapper that implements rate."
    def __init__(self, model_size, factor, warmup, optimizer):
        self.optimizer = optimizer
        self._step = 0
        self.warmup = warmup
        self.factor = factor
        self.model_size = model_size
        self._rate = 0
        
    def step(self):
        "Update parameters and rate"
        self._step += 1
        rate = self.rate()
        for p in self.optimizer.param_groups:
            p['lr'] = rate
        self._rate = rate
        self.optimizer.step()
        
    def rate(self, step = None):
        "Implement `lrate` above"
        if step is None:
            step = self._step
        return self.factor * \
            (self.model_size ** (-0.5) *
            min(step ** (-0.5), step * self.warmup ** (-1.5)))
        
def get_std_opt(model):
    return NoamOpt(model.src_embed[0].d_model, 2, 4000,
            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))

class LabelSmoothing(nn.Module):
    "Implement label smoothing."
    def __init__(self, size, padding_idx, smoothing=0.0):
        super(LabelSmoothing, self).__init__()
        self.criterion = nn.KLDivLoss(size_average=False)
        self.padding_idx = padding_idx
        self.confidence = 1.0 - smoothing
        self.smoothing = smoothing
        self.size = size
        self.true_dist = None
        
    def forward(self, x, target):
        assert x.size(1) == self.size
        true_dist = x.data.clone()
        true_dist.fill_(self.smoothing / (self.size - 2))
        true_dist.scatter_(1, target.data.unsqueeze(1).to(dtype=torch.int64), self.confidence)
        true_dist[:, self.padding_idx] = 0
        mask = torch.nonzero(target.data == self.padding_idx)
        if mask.dim() > 0:
            true_dist.index_fill_(0, mask.squeeze(), 0.0)
        self.true_dist = true_dist
        return self.criterion(x, Variable(true_dist, requires_grad=False))

class SimpleLossCompute:
    "A simple loss compute and train function."
    def __init__(self, generator, criterion, opt=None):
        self.generator = generator
        self.criterion = criterion
        self.opt = opt
        
    def __call__(self, x, y, norm):
        """
        norm 非pad的单词个数
        """
        x = self.generator(x)
        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)) / norm
        loss.backward()
        if self.opt is not None:
            self.opt.step()
            self.opt.optimizer.zero_grad()
        return loss.item() * norm

def run_epoch(data_iter, model, loss_compute):
    "Standard Training and Logging Function"
    start = time.time()
    total_tokens = 0
    total_loss = 0
    tokens = 0
    for i, batch in enumerate(data_iter):
        out = model.forward(batch.src, batch.trg, batch.src_mask, batch.trg_mask)
        loss = loss_compute(out, batch.trg_y, batch.ntokens)
        total_loss += loss
        total_tokens += batch.ntokens
        tokens += batch.ntokens
        if i % 50 == 1:
            elapsed = time.time() - start
            print("Epoch Step: %d Loss: %f Tokens per Sec: %f" %
                    (i, loss / batch.ntokens, tokens / elapsed))
            start = time.time()
            tokens = 0
    return total_loss / total_tokens


def data_gen(V, batch, nbatches):
    "Generate random data for a src-tgt copy task."
    """
    V : 词汇表长度
    nbatches : 有多少个batch
    batch：batch size
    """
    for i in range(nbatches):
        # 10 是句子长度
        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))
        data[:, 0] = 1
        src = Variable(data, requires_grad=False)
        tgt = Variable(data, requires_grad=False)
        yield Batch(src, tgt, 0)

V = 11  # vocabulary size
criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)
model = make_model(V, V, N=2)
model_opt = NoamOpt(model.src_embed[0].d_model, 1, 400,
        torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))

for epoch in range(10):
    model.train()
    run_epoch(data_gen(V, 30, 20), model, 
              SimpleLossCompute(model.generator, criterion, model_opt))
    model.eval()
    print(run_epoch(data_gen(V, 30, 5), model, 
                    SimpleLossCompute(model.generator, criterion, None)))
'''
o = torch.rand((5,6,7))
o[:,-1].shape

'''
def greedy_decode(model, src, src_mask, max_len, start_symbol):
    memory = model.encode(src, src_mask)
    batch_size, _ = src.shape
    ys = torch.ones(batch_size, 1).fill_(start_symbol).type_as(src.data)

    for i in range(max_len-1):
        out = model.decode(memory, src_mask, 
                           Variable(ys), 
                           Variable(subsequent_mask(ys.size(1))
                                    .type_as(src.data)))
        
        # prob = batch size, vocab size
        prob = model.generator(out[:, -1])
        _, next_word = torch.max(prob, dim = -1)
        
        ys = torch.cat([ys, next_word.unsqueeze(1)], dim=-1)
    return ys

src = Variable(torch.LongTensor([[1,2,3,4,5,6,7,8,9,10],
                                 [1,2,3,4,8,6,7,8,9,10]
                                ]) )
src_mask = Variable(torch.ones(2, 1, 10) )
print(src_mask)
print(greedy_decode(model, src, src_mask, max_len=10, start_symbol=1)[0])
# model.decode
from tqdm import tqdm
class Hypothesis:
    """
    保存beamsearch 的备选方案
    """

    def __init__(self, tokens, log_probs):
        self.tokens = tokens  # list of all the tokens from time 0 to the current time step t
        self.log_probs = log_probs  # list of the log probabilities of the tokens of the tokens

    def extend(self, token, log_prob):
        """
        Method to extend the current hypothesis by adding the next decoded token and all the informations associated with it
        """
        return Hypothesis(tokens=torch.cat([self.tokens, token]),  # we add the decoded token
                          log_probs=self.log_probs + [log_prob],  # we add the log prob of the decoded token
                          )

    @property
    def latest_token(self):
        return self.tokens[-1]

    @property
    def tot_log_prob(self):
        return sum(self.log_probs)

    @property
    def avg_log_prob(self):
        return self.tot_log_prob / len(self.tokens)

def get_top_k_beam(prediction, k):
    top_k_probs, top_k_ids = nn.functional.softmax(prediction, 0).topk(k)

    top_k_log_probs = torch.log(top_k_probs)
    return top_k_log_probs, top_k_ids

def beam_search(model, src, src_mask, max_len, start_symbol, beam_size, device):
    # memory = batch size, src len, d_model
    memory = model.encode(src, src_mask)
    batch_size, _ = src.shape
    all_tokens = []
    for batch in tqdm(range(batch_size)):
        batch_memory = memory[batch]
        batch_src_mask = src_mask[batch]

        tokens = beam_search_single_batch(model, batch_memory, batch_src_mask,
                                                  beam_size, start_symbol, max_len, device)
        
        all_tokens.append(tokens)
    all_tokens = torch.stack(all_tokens)
    return all_tokens

def beam_search_single_batch(model, memory, src_mask, beam_size, start_symbol, max_len, device):
    """
    memory = [src len, d_model]
    """
    # memory = [beam size, src len, d_model]
    memory = memory.unsqueeze(0).repeat(beam_size, 1, 1)
    hys = [Hypothesis(torch.LongTensor([start_symbol]).to(device), [0]) for _ in range(beam_size)]
    
    for i in range(max_len-1):
        # beam_input = [beam size, decoded len]
        beam_input = torch.stack([h.tokens for h in hys])
        trg_mask= subsequent_mask(beam_input.size(1)).type_as(memory.data)
        
        out = model.decode(memory, src_mask, beam_input, trg_mask)
        probs = model.generator(out[:,-1])  # beam size, vocab size
        
        # 把之前的最优解再扩展一个词，先要获取之前最优解的个数
        num_ori_hyps = 1 if i == 0 else len(hys)
 
        # 创建列表保存新的结果
        all_hyps = []
        
        # 遍历添加所有可能结果
        for j in range(num_ori_hyps):
            prob = probs[j, :]
            top_k_log_probs, top_k_ids = get_top_k_beam(prob, beam_size)
            
            for log_prob, top_k_id in zip(top_k_log_probs, top_k_ids):
                top_k_id = top_k_id.view(-1)
                new_hys = hys[j].extend(top_k_id, log_prob)
                all_hyps.append(new_hys)
        
        
        # 重置
        hys = []
        # 按照概率来排序
        sorted_hyps = sorted(all_hyps, key = lambda h:h.avg_log_prob, reverse=True)
        hys = sorted_hyps[:beam_size]
    
    tokens = hys[0].tokens
    return tokens

class MyDataset(data.Dataset):

    def __init__(self, datatuple, text_field, label_field, test=False):  # datatuple指的是元组('this moive is great',1)
        fields = [("text", text_field), ("label", label_field)]
        lists = []
        for content, label in tqdm(datatuple):
            # Example: Defines a single training or test example.Stores each column of the example as an attribute.
            lists.append(data.Example.fromlist([content, label], fields))
        # 之前是一些预处理操作，此处调用super初始化父类，构造自定义的Dataset类
        super().__init__(lists, fields)


def read_data(path):
    data_list = []
    with open(path,encoding="utf-8") as F:
        for line in F:
            line = line.split("\t")
            text = line[0]
            label = line[-1].strip().replace(' ','')
            data_list.append((text,label))
        return data_list
           
'''
train_data_lists
len(train_data_lists)
train_data_lists[0][0]
fields = [("text", SRC), ("label", LABEL)]
content = train_data_lists[10][0]
label = train_data_lists[10][1]
temp = data.Example.fromlist([content, label], fields)
print(temp)
dir(temp)
temp.label
temp.text
vo = SRC.build_vocab(train_dataset)
print(vo)
dir(vo)
LABEL.vocab.itos[int(5)]
'''

    
def prepare_data_iter(config):
    train_data_lists = read_data(config.training_path)
    test_data_list = read_data(config.testing_path)
    validation_data_list = read_data(config.validation_path)
    
    SRC = Field(tokenize=lambda x: x.split(), init_token=config.init_token, eos_token=config.eos_token,
                fix_length=config.fix_length)
    
    SRC = Field(tokenize=lambda x: x.split(), 
                fix_length=config.fix_length)

    LABEL = Field(tokenize=jieba.lcut, init_token=config.init_token, eos_token=config.eos_token,
                  fix_length=config.fix_length)

    train_dataset = MyDataset(train_data_lists, SRC, LABEL)
    test_dataset = MyDataset(test_data_list, SRC, LABEL)
    valid_dataset = MyDataset(validation_data_list,SRC,LABEL)

#     SRC.build_vocab(train_dataset,min_freq = 2)  # '<unk>':0,'<sos>':1,'i':2
#     LABEL.build_vocab(train_dataset,min_freq = 2)
    SRC.build_vocab(train_dataset)  
    LABEL.build_vocab(train_dataset)
    
    train_iter = data.BucketIterator(dataset=train_dataset, batch_size=config.batch_size,
                                     shuffle=True, sort_key=lambda x: len(x.text),
                                      sort_within_batch=False, repeat=False, device = config.device)

    test_iter = data.BucketIterator(dataset=test_dataset, batch_size=config.batch_size,
                                     shuffle=False, sort_key=lambda x: len(x.text),
                                      sort_within_batch=False, repeat=False, device = config.device)
    valid_iter = data.BucketIterator(dataset=valid_dataset, batch_size=config.batch_size,
                                     shuffle=False, sort_key=lambda x: len(x.text),
                                      sort_within_batch=False, repeat=False, device = config.device)
    return train_iter, test_iter, valid_iter, SRC, LABEL       

def tokens2sentence(preds, LABEL):
    sentences = []
    for tokens in preds:
        sentence = []
        for token in tokens:
            word = LABEL.vocab.itos[int(token.item())]
            
            if word == '<eos>':
                break
            sentence.append(word)
        sentences.append(sentence)

    return sentences

def computebleu(sentences, targets):
    score = 0
    count = len(sentences)
    assert (len(sentences) == len(targets))

    def cut_token(sentence):
        tmp = []
        for token in sentence:
            if token == "<eos>":
                break
            if token == '<unk>' or token.isdigit() or len(bytes(token[0], encoding='utf-8')) == 1:
                tmp.append(token)
            else:
                tmp += [word for word in token]
        return tmp

    for sentence, target in zip(sentences, targets):
        sentence = cut_token(sentence)
        target = cut_token(target)
        score += sentence_bleu([target], sentence, weights=(1, 0, 0, 0))

    return score, count

def run_epoch(data_iter, model, loss_compute, pad):
    """
    loss_compute：一个函数，包含全连接,计算损失，反向传播
    """
    start = time.time()
    total_tokens = 0
    total_loss = 0
    tokens = 0
    for i, ((src,trg),_) in enumerate(data_iter):
        src = src.permute(1,0)
        trg = trg.permute(1,0)
        batch = Batch(src, trg, pad)
        
        out = model.forward(batch.src, batch.trg, batch.src_mask, batch.trg_mask)
        loss = loss_compute(out, batch.trg_y, batch.ntokens)
        
        total_loss += loss
        total_tokens += batch.ntokens
        tokens += batch.ntokens
        if i % 50 == 1:
            elapsed = time.time() - start
            print("Epoch Step: %d Loss: %f Tokens per Sec: %f" %
                    (i, loss / batch.ntokens, tokens / elapsed))
            start = time.time()
            tokens = 0
            
    return total_loss / total_tokens

def train_process(config, SRC, LABEL):

    pad = SRC.vocab.stoi['<pad>']
    src_vocab = len(SRC.vocab)
    tgt_vocab = len(LABEL.vocab)

    N = config.n_layers
    d_model = config.d_model
    d_ff = config.d_ff

    h = config.h
    dropout = config.dropout
    device = config.device
    model = make_model(src_vocab, tgt_vocab, N, d_model, d_ff, h, dropout).to(device)
    
    max_len = config.fix_length
    start_symbol = LABEL.vocab.stoi['<bos>']
    beam_size = config.beam_size
    

    # 损失函数
    smoothing = config.smoothing
    criterion = LabelSmoothing(tgt_vocab, pad, smoothing)

    # 优化器
    factor = config.factor
    warmup = config.warmup
    optimizer = torch.optim.Adam(model.parameters(),lr=0, betas = (0.9, 0.98), eps=1e-9)
    model_opt = NoamOpt(d_model, factor, warmup, optimizer)

    best_score = 0
    # 封装计算损失函数的对象
    loss_compute = SimpleLossCompute(model.generator, criterion, model_opt)
    for i in range(config.epochs):
        
        run_epoch(train_iter, model, loss_compute, pad)
        total_score, total_num, _ = test(model, valid_iter, pad, max_len, start_symbol,device, beam_size)
        
        print('epoch{}:total bleu score:{:.4f}, total scentence:{},average score{:.4f}'.format(
            i,total_score,total_num,total_score/total_num))
        if total_score/total_num > best_score:
            best_score = total_score/total_num
            # 保存模型
            torch.save(model.state_dict(),'transformer.pth')
    return model
'''
i,((src,trg),_) = next(enumerate(test_iter))
src.shape
pad = SRC.vocab.stoi['<pad>']
src_mask = (src != pad)
src_mask.shape
src_mask = (src != pad).unsqueeze(-2)
scores = torch.rand((128,50,50)).to('cuda')
scores.to('cuda')
scores.device
src_mask.device
src_mask.to('cuda')
scores = scores.masked_fill(src_mask == 0, -1e9)
scores.shape
'''
def test(model, data_iter, pad, max_len, start_symbol, device, beam_size = 1):
    model = model
    total_score = 0
    total_count = 0
    results = []
    print('testing...')
    for (src, trg), _ in tqdm(data_iter):
        
        src = src.permute(1,0)
        trg = trg.permute(1,0)
        src_mask = (src != pad).unsqueeze(-2)
        
        if beam_size == 1:
            ids = greedy_decode(model, src, src_mask, max_len, start_symbol)
            sentences = tokens2sentence(ids, LABEL)
            # label_text = tokens2sentence(ids, LABEL)
        else:
            ids = beam_search(model, src, src_mask, max_len, start_symbol, beam_size, device)
            sentences = tokens2sentence(ids, LABEL)
            
        targets = tokens2sentence(trg, LABEL)
        source = tokens2sentence(src, SRC)
#         results.append((source,targets,sentences))
        score, count = computebleu(sentences, targets)
        for src, tgt, pred in zip(source, targets,sentences):
            results.append((list(filter(lambda x: x!= '<pad>', src)), tgt, pred ))
        total_score += score
        total_count += count
    return total_score, total_count, results
class Config:
    def __init__(self):
        self.data_path = r"E:\work\股价预测\data"
        self.testing_path = self.data_path + '/testing.txt'
        self.training_path = self.data_path + '/training.txt'
        self.validation_path = self.data_path + '/validation.txt'
        self.init_token = '<bos>'
        self.eos_token = '<eos>'
        self.fix_length = 50
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.batch_size = 128
        self.beam_size = 4
        self.n_layers = 6
        self.h = 8
        self.d_model = 512
        self.d_ff = 2048
        self.dropout = 0.1
        self.epochs = 20
        self.load_model = False               
        self.load_model_path = 'weight.ckpt'     
        self.smoothing = 0
        self.factor = 2
        self.warmup = 40
        self.load_path = '../input/transformer-translate-entoch/transformer.pth'

              
config = Config()
config.beam_size = 1
train_iter, test_iter, valid_iter, SRC, LABEL = prepare_data_iter(config)


# 训练
model = train_process(config, SRC, LABEL)
# 打印参数个数
num_params = sum(p.numel() for p in model.parameters())
print(f'模型参数的总数: {num_params}')

config.load_path = 'transformer.pth'
# config = Config()
# # 加载训练好的模型做预测
# model = load_model(config, LABEL, SRC)
config.beam_size = 3
    
pad = SRC.vocab.stoi['<pad>']
max_len = config.fix_length
start_symbol = LABEL.vocab.stoi['<bos>']
beam_size = config.beam_size
device = config.device
total_score, total_scentence, res = test(model, test_iter, pad, max_len, start_symbol, device, beam_size )
'''
len(res)
len(test_data_list)
i,(s,t) =next(enumerate(test_iter))
src[1].shape
r[0]
i,((src, trg), _ )= next(enumerate(test_iter))
res[35]
src.shape
s
len(s)
s[0].shape
s[1].shape
t
print()
model
sum(p.numel() for p in model.parameters() if p.requires_grad)
'''

'''
# 保存模型的状态字典
torch.save(model.state_dict(), r'E:\work\股价预测\data\model\trained_model.pth')
model = make_model(src_vocab, tgt_vocab, N, d_model, d_ff, h, dropout).to(device)
# 加载模型
model_1 = make_model(src_vocab, tgt_vocab, N, d_model, d_ff, h, dropout).to(device)
model_1.load_state_dict(torch.load(r'E:\work\股价预测\data\model\trained_model.pth'))
# 输入语句
str = 'london is a capital of china ?'
str_list = str.split()
word_list = []
for token in str_list:
    word = SRC.vocab.stoi[token]
    word_list.append(word)
l = len(word_list)
# 推理，第一步
src = torch.tensor(word_list).to('cuda')
src = src.unsqueeze(0)
src_mask = Variable(torch.ones(1, 1, l)).to('cuda')
max_len = 6
ids = greedy_decode(model, src, src_mask, max_len, start_symbol)
sentences = tokens2sentence(ids, LABEL)
print(sentences)    
res[610]
'''
