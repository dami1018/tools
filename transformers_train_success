#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Nov 18 21:59:41 2024

@author: qinpeng
"""


from transformers import AutoTokenizer
from datasets import load_dataset
from transformers import AutoModelForSequenceClassification
import torch
# 加载中文的预训练模型tokenizer
tokenizer = AutoTokenizer.from_pretrained('hfl/rbt3')
# 加载数据，处理数据
dataset = load_dataset("ndiy/ChnSentiCorp")
# 先shuffle必须dataset改变，先map可以改变变量
'''
dataset['train'] = dataset['train'].shuffle().select(range(2000))
dataset['test'] = dataset['test'].shuffle().select(range(100))
'''
# 直接用分词处理有问题，需要再加载模型
# 处理报错，text报错，可能原因是不同的模型，bert-base-cased可以，但是hfl/rbt3不行，也可能是dataset的问题
# 不是不同的模型而是map调用时参数过多只有batched参数就没问题
def f(data):
    # from transformers import AutoTokenizer
    # tokenizer = AutoTokenizer.from_pretrained('hfl/rbt3')
    # return tokenizer.batch_encode_plus(data['text'], truncation=True)
    return tokenizer(data['text'], padding='max_length',truncation=True) 
'''
def tokenizer_function(data):
    return tokenizer(data['text'],padding='max_length',truncation=True)
tokenizer_dataset = dataset.map(tokenizer_function,batched=True,remove_columns=['text'])
'''
tokenizer_dataset = dataset.map(f,
                      batched=True,remove_columns=['text']) 
'''
                      ,
                      batch_size=1000,
                      num_proc=4,
                      remove_columns=['text'])
'''
small_train = tokenizer_dataset['train'].shuffle().select(range(2000))
small_test = tokenizer_dataset['test'].shuffle().select(range(100))
# 看似结果返回列表实际返回的是dataset类型
def ff(data):
    return [len(i) <= 512 for i in data['input_ids']]
# 过滤掉超过512个token的句子
small_train = small_train.filter(ff, batched=True, batch_size=1000, num_proc=4)
# 加载用于分类任务的预训练模型
model = AutoModelForSequenceClassification.from_pretrained('hfl/rbt3',
                                                           num_labels=2)
# sum([i.nelement() for i in model.parameters()]) / 10000
# 第6章/模型试算
# import torch
# 模拟一批数据
# 可以看出来，分类模型需要以torch即pt作为输入，输出为torch同时还输出loss函数
'''
data = {
    'input_ids': torch.ones(4, 10, dtype=torch.long),
    'token_type_ids': torch.ones(4, 10, dtype=torch.long),
    'attention_mask': torch.ones(4, 10, dtype=torch.long),
    'labels': torch.ones(4, dtype=torch.long)
}
out = model(**data)
print(out)
# 统计模型参数量
sum([i.nelement() for i in model.parameters()]) / 10000
'''
# 计算预测正确的函数

import numpy as np
from transformers.trainer_utils import EvalPrediction
# 定义计算指标的函数
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    logits = logits.argmax(axis=1)
    return {'accuracy': (logits == labels).sum() / len(labels)}
# 模拟输出
eval_pred = EvalPrediction(
    predictions=np.array([[0, 1], [2, 3], [4, 5], [6, 7]]),
    label_ids=np.array([1, 1, 0, 1]),
)
compute_metrics(eval_pred)

import os
os.getcwd()
# 第6章/定义训练参数
# mac会限制一些路径的读写权限 system 用户
from transformers import TrainingArguments
# 定义训练参数
args = TrainingArguments(
    # 定义临时数据保存路径
    output_dir='/Library/train',
    # 定义测试执行的策略，可取值no、epoch、steps
    evaluation_strategy='steps',
    # 定义每隔多少个step执行一次测试
    eval_steps=30,
    # 定义模型保存策略，可取值no、epoch、steps
    save_strategy='steps',
    # 定义每隔多少个step保存一次
    save_steps=30,
    # 定义共训练几个轮次
    num_train_epochs=1,
    # 定义学习率
    learning_rate=1e-4,
    # 加入参数权重衰减，防止过拟合
    weight_decay=1e-2,
    # 定义测试和训练时的批次大小
    per_device_eval_batch_size=16,
    per_device_train_batch_size=16,
    # 定义是否要使用gpu训练
    no_cuda=True,
)


# /Library /Library/train
from transformers import Trainer
from transformers.data.data_collator import DataCollatorWithPadding

# 定义训练器
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=small_train,
    eval_dataset=small_test,
    compute_metrics=compute_metrics,
    data_collator=DataCollatorWithPadding(tokenizer),
)
# 第6章/测试数据整理函数
data_collator = DataCollatorWithPadding(tokenizer)
# 获取一批数据
data = small_train[:5]
# 输出这些句子的长度
for i in data['input_ids']:
    print(len(i))
# 调用数据整理函数
data = data_collator(data)

# 查看整理后的数据
for k, v in data.items():
    print(k, v.shape)
# 解码第一个句子
tokenizer.decode(data['input_ids'][0])
# 第6章/评价模型
trainer.evaluate()
# 第6章/训练
trainer.train()




print(tokenizer_dataset)
print(tokenizer)
print(dataset)
print(small_train)
