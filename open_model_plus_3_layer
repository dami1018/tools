#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 28 07:30:57 2025

@author: qinpeng
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
# 1、模型定义
def self_attention(q,k,v):
    dim = torch.tensor(q.size(-1))
    weight = q @ k.transpose(-2,-1)/torch.sqrt(dim)
    weight_prob = F.log_softmax(weight,dim=-1)
    return weight_prob @ v
class multi_attention(nn.Module):
    def __init__(self,head,embed_dim,dropout):
        super(multi_attention,self).__init__()
        self.fc = nn.ModuleList(nn.Linear(embed_dim, embed_dim) for _ in range(4))
        self.head_dim = embed_dim // head
        self.head = head
        self.embed_dim = embed_dim
        self.dropout = nn.Dropout(p=dropout)
    def forward(self,q,k,v):
        batch_num = q.size(0)
        q,k,v = [ l(x).view(batch_num,-1,self.head,self.head_dim).transpose(1,2) for l,x in zip(self.fc,(q,k,v))]
        x = self_attention(q, k, v)
        x = x.transpose(1,2).contiguous().view(batch_num,-1,self.head*self.head_dim)
        return self.fc[-1](x)
class feedforward(nn.Module):
    def __init__(self,embed_dim,hidden_dim,dropout=0.1):
        super(feedforward,self).__init__()
        self.lin1 = nn.Linear(embed_dim, hidden_dim)
        self.lin2 = nn.Linear(hidden_dim, embed_dim)
        self.dropout = nn.Dropout(p=dropout)
    def forward(self,x):
        return self.lin2(self.dropout(F.relu(self.lin1(x))))
class layernorm(nn.Module):
    def __init__(self,embed_dim,eps=1e-6):
        super(layernorm,self).__init__()
        self.w = nn.Parameter(torch.ones(embed_dim))
        self.b = nn.Parameter(torch.zeros(embed_dim))
        self.eps = eps
    def forward(self,x):
        mean = x.mean(dim=-1,keepdim=True)
        std = x.std(dim=-1,keepdim=True)
        return self.w * ((x - mean) / (std + self.eps)) + self.b
class layernorm_plus(nn.Module):
    def __init__(self,eps=1e-6):
        super(layernorm_plus,self).__init__()
        self.eps = eps
    def forward(self,x):
        mean = x.mean(dim=-1,keepdim=True)
        std = x.std(dim=-1,keepdim=True)
        return ((x - mean) / (std + self.eps)) 
class generator(nn.Module):
    def __init__(self,embed_dim,out_dim):
        super(generator,self).__init__()
        self.fc = nn.Linear(embed_dim, out_dim)
    def forward(self,x):
        return F.log_softmax(self.fc(x).squeeze(dim=-1),dim=-1)
class generator_plus(nn.Module):
    def __init__(self,embed_dim,out_dim):
        super(generator_plus,self).__init__()
        self.fc = nn.Linear(embed_dim, out_dim)
        self.layernorm = layernorm_plus()
    def forward(self,x):
        return self.layernorm(self.fc(x).squeeze(dim=-1))
class encoderlayer(nn.Module):
    def __init__(self,head,embed_dim,hidden_dim,dropout=0.1):
        super(encoderlayer,self).__init__()
        self.norm = nn.ModuleList(layernorm(embed_dim) for _ in range(2))
        self.attention = multi_attention(head, embed_dim, dropout)
        self.dropout = nn.Dropout(p=dropout)
        self.feedforward = feedforward(embed_dim, hidden_dim)
    def forward(self,x):
        x_norm = self.norm[0](x)
        x_bias = self.attention(x_norm,x_norm,x_norm)
        x_1 = x_norm + self.dropout(x_bias)
        x_norm_feed = self.norm[1](x_1) 
        x_feed_bias = self.feedforward(x_norm_feed)
        x_2 = x_norm_feed + self.dropout(x_feed_bias)
        return(x_2)
class Encoder(nn.Module):
    def __init__(self,layer_num,head,embed_dim,hidden_dim,out_dim=1,dropout=0.1):
        super(Encoder,self).__init__()
        self.encoder = nn.ModuleList(encoderlayer(head, embed_dim, hidden_dim) for _ in range(layer_num))
        self.generator = generator(embed_dim,out_dim)
    def forward(self,x):
        x_1 = self.encoder[0](x)
        x_2 = self.encoder[1](x_1)
        return self.generator((x_2))
class Encoder_plus(nn.Module):
    def __init__(self,layer_num,head,embed_dim,hidden_dim,out_dim=1,dropout=0.1):
        super(Encoder_plus,self).__init__()
        self.encoder = nn.ModuleList(encoderlayer(head, embed_dim, hidden_dim) for _ in range(layer_num))
        self.generator = generator_plus(embed_dim,out_dim)
    def forward(self,x):
        x_1 = self.encoder[0](x)
        x_2 = F.relu(x_1)
        x_3 = self.encoder[1](x_2)
        x_4 = F.relu(x_3)
        x_5 = self.encoder[2](x_4)
        return self.generator((x_5))
def distance_simple(x,y):
    loss = torch.tensor(0,dtype=torch.float32)
    x = x.view(-1,x.size(-1))
    y = y.view(-1,y.size(-1))
    # res = []
    for k in range(x.size(0)):
        for i in range(x.size(1)):
            for j in range(i+1,x.size(1)):
                # res.append((k,i,j,loss.item()))
                if (~torch.isnan(x[k,i]))&(~torch.isnan(x[k,j])):
                    loss += F.relu(-(x[k,i] - x[k,j])*(y[k,i]-y[k,j])/torch.abs(y[k,i]-y[k,j]))            
    # return loss
    return (2*loss/(x.size(1)*(x.size(1)-1)*x.size(0)))
def distance_simple_improved(x,y):
    loss = torch.tensor(0,dtype=torch.float32)
    x = x.view(-1,x.size(-1))
    y = y.view(-1,y.size(-1))
    # res = []
    for k in range(x.size(0)):
        for i in range(x.size(1)):
            for j in range(i+1,x.size(1)):
                # res.append((k,i,j,loss.item()))
                if (~torch.isnan(x[k,i]))&(~torch.isnan(x[k,j])):
                    loss += F.relu(-(x[k,i] - x[k,j])*(y[k,i]-y[k,j])/torch.abs(y[k,i]-y[k,j]))            
    # return loss
    return (2*loss/(x.size(1)*(x.size(1)-1)*x.size(0)))

# 2、数据导入,是numpy类型的，使用时要转化为tensor
data_group_in = np.load(r'/Users/qinpeng/Desktop/python项目/天软/node_jfkd_input.npy')
y_sort = np.load(r'/Users/qinpeng/Desktop/python项目/天软/node_jfkd_target.npy')

# 3、训练函数
epoch = 500
batch_size = 25
length = data_group_in.shape[0]
head = 1
embed_dim = 1210
dropout = 0.1
hidden_dim = 256
layer_num = 3
lr = 0.01
num = length // batch_size
encoder_plus = Encoder_plus(layer_num, head, embed_dim, hidden_dim)
optimizer_plus = optim.Adam(encoder_plus.parameters(),lr=0.001)

parameter_list = []
stk_len = 31
for i in range(epoch): 
    for j in range(num-1):
        data_group_in_pt = torch.tensor(data_group_in[j*batch_size:(j+1)*batch_size,:,:],dtype=torch.float32)
        y_sort_pt = torch.tensor(y_sort[j*batch_size:(j+1)*batch_size,:],dtype=torch.float32)
        encoder_plus.train()
        optimizer_plus.zero_grad()
        y_pred = encoder_plus(data_group_in_pt)
        loss = distance_simple(y_pred, y_sort_pt)
        # loss.requires_grad_(True)
        loss.backward()
        optimizer_plus.step()
        fc_grad = 0
        else_grad = 0
        for name,parameter in encoder_plus.named_parameters():
            parameter_list.append((name,tuple(parameter.grad.data.shape),parameter.grad.data.norm().item()))
            if name=='generator.fc.weight':
                fc_grad += parameter.grad.norm()
            else:
                else_grad += parameter.grad.norm()
        with torch.no_grad():
            x_test = torch.tensor(data_group_in[225:226,:,:],dtype=torch.float32)
            out_test = encoder_plus(x_test)
            y_test = torch.tensor(y_sort[225:226,:],dtype=torch.float32)
            predict_loss = distance_simple(out_test, y_test)
            random_loss = 0
            for k in range(100):
                temp = torch.range(0, (stk_len-1))
                temp = temp[torch.randperm(temp.size(0))]
                temp = temp.unsqueeze(dim=0)
                random_loss += distance_simple(out_test, temp)
            random_loss = random_loss / 100
        # torch.set_printoptions(precision=6,  sci_mode=False)
        print(f"Epoch {i}, Step {j},Loss: {loss.item():.6f},predict_loss: {predict_loss:.6f},random_loss: {random_loss:.6f},fc: {fc_grad:.6f},else: {else_grad:.6f}")

# 4、测试检验
state_dict = encoder_plus.state_dict()
state_dict['generator.fc.weight'].norm()
state_dict['generator.fc.weight'].grad.data
for name,parameter in encoder_plus.named_parameters():
    if name=='generator.fc.weight':
        print(parameter.grad)
# 5、分布的改变
# 可以直接使用dict
para_list = []
for name,para in encoder_plus.named_parameters():
    para_list.append((name,para))
state_dict = encoder_plus.state_dict()
para_sample = state_dict['encoder.0.attention.fc.0.weight']
# para 没训练之前
import matplotlib.pyplot as plt
from scipy.stats import norm
para_flatten = para_sample.reshape(-1)
mean = para_sample.mean()
std = para_sample.std()
plt.hist(para_flatten,bins=100,density=True,alpha=0.6,color='g')
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 100)
p = norm.pdf(x, mean, std)
plt.plot(x, p, 'k', linewidth=2, label='normal')
plt.show()
# para 训练之后
state_dict = encoder_plus.state_dict()
para_sample = state_dict['encoder.0.attention.fc.0.weight']
para_flatten = para_sample.reshape(-1)
mean = para_sample.mean()
std = para_sample.std()
plt.hist(para_flatten,bins=100,density=True,alpha=0.6,color='g')
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 100)
p = norm.pdf(x, mean, std)
plt.plot(x, p, 'k', linewidth=2, label='normal')
plt.show()
#
input_dict = {}
output_dict = {}
def hook_fn(module, input, output):
    input_dict[module] = input
    output_dict[module] = output
handle = encoder_plus.encoder[0].attention.fc[0].register_forward_hook(hook_fn)
b = torch.tensor(data_group_in[0:2,:,:],dtype=torch.float32)
y = encoder_plus(b)
handle.remove()
# input
keys = list(input_dict.keys())
sample = input_dict[keys[0]][0]
mean = input_dict[keys[0]][0].mean().item()
std = input_dict[keys[0]][0].std().item()
sample = sample.detach().numpy()
sample_flatten = sample.reshape(-1)
plt.hist(sample_flatten,bins=100,density=True,alpha=0.6,color='g')
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 100)
p = norm.pdf(x, mean, std)
plt.plot(x, p, 'k', linewidth=2, label='normal')
plt.show()
# output
keys = list(input_dict.keys())
sample = output_dict[keys[0]]
mean = output_dict[keys[0]].mean().item()
std = output_dict[keys[0]].std().item()
sample = sample.detach().numpy()
sample_flatten = sample.reshape(-1)
plt.hist(sample_flatten,bins=100,density=True,alpha=0.6,color='g')
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 100)
p = norm.pdf(x, mean, std)
plt.plot(x, p, 'k', linewidth=2, label='normal')
plt.show()
# 过拟合的参数是正态的吗？
model = torch.load('encoder_0124.pth')
dict_model = model.state_dict()
keys = dict_model.keys()
for key in keys:
    print(key)
para_pretrained = dict_model['encoder.1.attention.fc.3.weight']
para_flatten = para_pretrained.reshape(-1)
mean = para_sample.mean()
std = para_sample.std()
plt.hist(para_flatten,bins=100,density=True,alpha=0.6,color='g')
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 100)
p = norm.pdf(x, mean, std)
plt.plot(x, p, 'k', linewidth=2, label='normal')
plt.show()

# 最大值最小值分析
para_pretrained = dict_model['encoder.0.attention.fc.0.weight']
para_pretrained.max()
max_list = torch.argwhere(para_pretrained>0.3)
rows,cols = max_list[:,0],max_list[:,1]
para_pretrained[rows,cols]
para_pretrained[max_list[0]]
min_list = torch.argwhere(para_pretrained<-0.3)
rows,cols = min_list[:,0],min_list[:,1]
para_pretrained[rows,cols]
# 
lin = nn.Linear(1024, 1024)
lin_dict = lin.state_dict()
lin_par = lin_dict['weight']
para_flatten = lin_par.reshape(-1)
mean = para_sample.mean()
std = para_sample.std()
plt.hist(para_flatten,bins=100,density=True,alpha=0.6,color='g')
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 100)
p = norm.pdf(x, mean, std)
plt.plot(x, p, 'k', linewidth=2, label='normal')
plt.show()

# grad
for name,para in encoder_plus.named_parameters():
    if name == 'encoder.0.attention.fc.0.weight':
        data = para.grad.data
para_flatten = data.reshape(-1)
mean = data.mean()
std = data.std()
plt.hist(para_flatten,bins=100,density=True,alpha=0.6,color='g')
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 100)
p = norm.pdf(x, mean, std)
plt.plot(x, p, 'k', linewidth=2, label='normal')
plt.show()

# para
for name,parameter in encoder_plus.named_parameters():
    if name== 'encoder.0.attention.fc.0.weight':
        mean = parameter.data.mean()
        std = parameter.data.std()
        temp = torch.randn_like(parameter.data) * std *1.4  + mean + 0.1
        parameter.data = temp
        shape = parameter.data.shape
        break
    
for name,para in encoder_plus.named_parameters():
    if name == 'encoder.0.attention.fc.0.weight':
        data = para.data
para_flatten = data.reshape(-1)
mean = data.mean()
std = data.std()
plt.hist(para_flatten,bins=100,density=True,alpha=0.6,color='g')
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 100)
p = norm.pdf(x, mean, std)
plt.plot(x, p, 'k', linewidth=2, label='normal')
plt.show()

# 
class lin_squre(nn.Module):
    def __init__(self,in_features=256,out_features=256):
        super(lin_squre,self).__init__()
        self.lin = nn.Linear(in_features, out_features)
    def forward(self,x):
        x1 = self.lin(x)
        return(self.lin(x1))
    
x = torch.randn(256,256)    
lin = lin_squre()
lin(x)    
loss = (lin(x) - torch.ones_like(x)).mean()
loss.backward()
par = lin.state_dict()
# 错误的引用方式
for name,data in par.items():
    print(name,data.grad.mean())
# 正确的引用方式
for name,parameter in lin.named_parameters():
    print(name,parameter.grad.data.mean())

# 二次的模型
lin = lin_squre()    
x = torch.randn(256,256)
y = torch.ones((256,256))
epoch = 500
criterion = nn.MSELoss()   
optimizer = optim.Adam(lin.parameters(),lr=0.001)
for i in range(epoch):
    lin.train()
    optimizer.zero_grad()
    output = lin(x)  
    loss = criterion(output,y)
    loss.backward() 
    optimizer.step()
    print(i,loss)
    


    
class MLP(nn.Module):
    def __init__(self,in_features=1210,out_features=1210):
        super(MLP,self).__init__()
        self.lin = nn.Linear(in_features, out_features)
        self.relu = nn.ReLU()
        self.lin2 = nn.Linear(in_features, 1)
        self.layernorm = layernorm_plus()
    def forward(self,x):
        x1 = self.relu(self.lin(x))
        x2 = self.lin(x1)
        x3 = self.lin2(x2)
        return self.layernorm(x3.squeeze(dim=-1))
data_group_in = np.load(r'/Users/qinpeng/Desktop/python项目/天软/node_jfkd_input.npy')
y_sort = np.load(r'/Users/qinpeng/Desktop/python项目/天软/node_jfkd_target.npy') 
model = MLP()
stk_len = 31
optimizer = optim.Adam(model.parameters(),lr=0.001)
for i in range(epoch): 
    for j in range(num-1):
        data_group_in_pt = torch.tensor(data_group_in[j*batch_size:(j+1)*batch_size,:,:],dtype=torch.float32)
        y_sort_pt = torch.tensor(y_sort[j*batch_size:(j+1)*batch_size,:],dtype=torch.float32)
        model.train()
        optimizer_plus.zero_grad()
        y_pred = model(data_group_in_pt)
        loss = distance_simple(y_pred, y_sort_pt)
        # loss.requires_grad_(True)
        loss.backward()
        optimizer_plus.step()
        else_grad = 0
        for name,parameter in model.named_parameters():
            parameter_list.append((name,tuple(parameter.grad.data.shape),parameter.grad.data.norm().item()))
            else_grad += parameter.grad.norm()
        with torch.no_grad():
            x_test = torch.tensor(data_group_in[225:226,:,:],dtype=torch.float32)
            out_test = encoder_plus(x_test)
            y_test = torch.tensor(y_sort[225:226,:],dtype=torch.float32)
            predict_loss = distance_simple(out_test, y_test)
            random_loss = 0
            for k in range(100):
                temp = torch.range(0, (stk_len-1))
                temp = temp[torch.randperm(temp.size(0))]
                temp = temp.unsqueeze(dim=0)
                random_loss += distance_simple(out_test, temp)
            random_loss = random_loss / 100
        # torch.set_printoptions(precision=6,  sci_mode=False)
        print(f"Epoch {i}, Step {j},Loss: {loss.item():.6f},predict_loss: {predict_loss:.6f},random_loss: {random_loss:.6f},all_grad: {else_grad:.6f}")




class MLP(nn.Module):
    def __init__(self,in_features=1210,out_features=1210):
        super(MLP,self).__init__()
        self.lin = nn.Linear(in_features, out_features)
        self.lin2 = nn.Linear(in_features, 1)
        self.layernorm = layernorm_plus()
    def forward(self,x):
        x1 = self.lin(x)
        x2 = self.lin(x1)
        x3 = self.lin2(x2)
        return self.layernorm(x3.squeeze(dim=-1))
data_group_in = np.load(r'/Users/qinpeng/Desktop/python项目/天软/node_jfkd_input.npy')
y_sort = np.load(r'/Users/qinpeng/Desktop/python项目/天软/node_jfkd_target.npy') 
model = MLP()
stk_len = 31
optimizer = optim.Adam(model.parameters(),lr=0.001)
for i in range(epoch): 
    for j in range(num-1):
        data_group_in_pt = torch.tensor(data_group_in[j*batch_size:(j+1)*batch_size,:,:],dtype=torch.float32)
        y_sort_pt = torch.tensor(y_sort[j*batch_size:(j+1)*batch_size,:],dtype=torch.float32)
        model.train()
        optimizer.zero_grad()
        y_pred = model(data_group_in_pt)
        loss = distance_simple(y_pred, y_sort_pt)
        # loss.requires_grad_(True)
        loss.backward()
        optimizer.step()
        else_grad = 0
        for name,parameter in model.named_parameters():
            else_grad += parameter.grad.norm()
        with torch.no_grad():
            x_test = torch.tensor(data_group_in[225:226,:,:],dtype=torch.float32)
            out_test = encoder_plus(x_test)
            y_test = torch.tensor(y_sort[225:226,:],dtype=torch.float32)
            predict_loss = distance_simple(out_test, y_test)
            random_loss = 0
            for k in range(100):
                temp = torch.range(0, (stk_len-1))
                temp = temp[torch.randperm(temp.size(0))]
                temp = temp.unsqueeze(dim=0)
                random_loss += distance_simple(out_test, temp)
            random_loss = random_loss / 100
        # torch.set_printoptions(precision=6,  sci_mode=False)
        print(f"Epoch {i}, Step {j},Loss: {loss.item():.6f},predict_loss: {predict_loss:.6f},random_loss: {random_loss:.6f},all_grad: {else_grad:.6f}")

    
model.state_dict()    
for name,para in model.named_parameters():
    if name == 'lin.weight':
        data = para.data
        
para_flatten = data.reshape(-1)
mean = data.mean()
std = data.std()
plt.hist(para_flatten,bins=100,density=True,alpha=0.6,color='g')
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 100)
p = norm.pdf(x, mean, std)
plt.plot(x, p, 'k', linewidth=2, label='normal')
plt.show()   
    
    
for name,para in model.named_parameters():
    if name == 'lin.weight':
        data = para.grad.data
para_flatten = data.reshape(-1)
mean = data.mean()
std = data.std()
plt.hist(para_flatten,bins=100,density=True,alpha=0.6,color='g')
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 100)
p = norm.pdf(x, mean, std)
plt.plot(x, p, 'k', linewidth=2, label='normal')
plt.show()      
    
data.norm()
torch.square((data * data).sum())   
    
data1 = data    
grad1 = data   

data2 = data
grad2 = data    

data1 - data2 

for name,para in model.named_parameters():
    if name == 'lin.weight':
        print(para.requires_grad)  


code = 'x=1  \ny=1' 
code = '''x=1
y=1''' 
variable = '''self.lin2 = nn.Linear(in_features, 1)'''
code = f'''class MLP1(nn.Module):
    def __init__(self,in_features=1210,out_features=1210):
        super(MLP,self).__init__()
        self.lin = nn.Linear(in_features, out_features)
        {variable}
        self.layernorm = layernorm_plus()
    def forward(self,x):
        x1 = self.lin(x)
        x2 = self.lin(x1)
        x3 = self.lin2(x2)
        return self.layernorm(x3.squeeze(dim=-1))'''      
exec(code)        



class MLP(nn.Module):
    def __init__(self,in_features=1210,out_features=1210):
        super(MLP,self).__init__()
        self.lin = nn.Linear(in_features, out_features)
        self.lin2 = nn.Linear(in_features, 1)
        self.relu = nn.ReLU()
        self.layernorm = layernorm_plus()
    def forward(self,x):
        x1 = self.relu(self.lin(x))
        x2 = self.relu(self.lin(x1))
        x3 = self.lin2(x2)
        return self.layernorm(x3.squeeze(dim=-1))
data_group_in = np.load(r'/Users/qinpeng/Desktop/python项目/天软/node_jfkd_input.npy')
y_sort = np.load(r'/Users/qinpeng/Desktop/python项目/天软/node_jfkd_target.npy') 
model = MLP()
stk_len = 31
optimizer = optim.Adam(model.parameters(),lr=0.001)
for i in range(epoch): 
    for j in range(num-1):
        data_group_in_pt = torch.tensor(data_group_in[j*batch_size:(j+1)*batch_size,:,:],dtype=torch.float32)
        y_sort_pt = torch.tensor(y_sort[j*batch_size:(j+1)*batch_size,:],dtype=torch.float32)
        model.train()
        optimizer.zero_grad()
        y_pred = model(data_group_in_pt)
        loss = distance_simple(y_pred, y_sort_pt)
        # loss.requires_grad_(True)
        loss.backward()
        optimizer.step()
        else_grad = 0
        for name,parameter in model.named_parameters():
            else_grad += parameter.grad.norm()
        with torch.no_grad():
            x_test = torch.tensor(data_group_in[225:226,:,:],dtype=torch.float32)
            out_test = encoder_plus(x_test)
            y_test = torch.tensor(y_sort[225:226,:],dtype=torch.float32)
            predict_loss = distance_simple(out_test, y_test)
            random_loss = 0
            for k in range(100):
                temp = torch.range(0, (stk_len-1))
                temp = temp[torch.randperm(temp.size(0))]
                temp = temp.unsqueeze(dim=0)
                random_loss += distance_simple(out_test, temp)
            random_loss = random_loss / 100
        # torch.set_printoptions(precision=6,  sci_mode=False)
        print(f"Epoch {i}, Step {j},Loss: {loss.item():.6f},predict_loss: {predict_loss:.6f},random_loss: {random_loss:.6f},all_grad: {else_grad:.6f}")

torch.save(model,'MLP_0130.pth')  


class MLP(nn.Module):
    def __init__(self,in_features=1210,out_features=1210):
        super(MLP,self).__init__()
        self.lin = nn.Linear(in_features, out_features)
        self.lin0 = nn.Linear(in_features, out_features)
        self.lin1 = nn.Linear(in_features, out_features)
        self.lin2 = nn.Linear(in_features, 1)
        self.relu = nn.ReLU()
        self.layernorm = layernorm_plus()
    def forward(self,x):
        x0 = self.relu(self.lin(x))
        x1 = self.relu(self.lin0(x0))
        x2 = self.relu(self.lin1(x1))
        x3 = self.lin2(x2)
        return self.layernorm(x3.squeeze(dim=-1))
data_group_in = np.load(r'/Users/qinpeng/Desktop/python项目/天软/node_jfkd_input.npy')
y_sort = np.load(r'/Users/qinpeng/Desktop/python项目/天软/node_jfkd_target.npy') 
model = MLP()
stk_len = 31
optimizer = optim.Adam(model.parameters(),lr=0.001)
for i in range(epoch): 
    for j in range(num-1):
        data_group_in_pt = torch.tensor(data_group_in[j*batch_size:(j+1)*batch_size,:,:],dtype=torch.float32)
        y_sort_pt = torch.tensor(y_sort[j*batch_size:(j+1)*batch_size,:],dtype=torch.float32)
        model.train()
        optimizer.zero_grad()
        y_pred = model(data_group_in_pt)
        loss = distance_simple(y_pred, y_sort_pt)
        # loss.requires_grad_(True)
        loss.backward()
        optimizer.step()
        else_grad = 0
        for name,parameter in model.named_parameters():
            else_grad += parameter.grad.norm()
        with torch.no_grad():
            x_test = torch.tensor(data_group_in[225:226,:,:],dtype=torch.float32)
            out_test = encoder_plus(x_test)
            y_test = torch.tensor(y_sort[225:226,:],dtype=torch.float32)
            predict_loss = distance_simple(out_test, y_test)
            random_loss = 0
            for k in range(100):
                temp = torch.range(0, (stk_len-1))
                temp = temp[torch.randperm(temp.size(0))]
                temp = temp.unsqueeze(dim=0)
                random_loss += distance_simple(out_test, temp)
            random_loss = random_loss / 100
        # torch.set_printoptions(precision=6,  sci_mode=False)
        print(f"Epoch {i}, Step {j},Loss: {loss.item():.6f},predict_loss: {predict_loss:.6f},random_loss: {random_loss:.6f},all_grad: {else_grad:.6f}")


    
class MLP(nn.Module):
    def __init__(self,in_features=1210,out_features=1210):
        super(MLP,self).__init__()
        self.lin = nn.Linear(in_features, out_features)
        self.lin0 = nn.Linear(in_features, out_features)
        self.lin1 = nn.Linear(in_features, out_features)
        self.lin2 = nn.Linear(in_features, out_features)
        self.lin3 = nn.Linear(in_features, 1)
        self.relu = nn.ReLU()
        self.layernorm = layernorm_plus()
    def forward(self,x):
        x0 = self.relu(self.lin(x))
        x1 = self.relu(self.lin0(x0))
        x2 = self.relu(self.lin1(x1))
        x3 = self.relu(self.lin2(x2))
        x4 = self.lin3(x3)
        return self.layernorm(x4.squeeze(dim=-1))
data_group_in = np.load(r'/Users/qinpeng/Desktop/python项目/天软/node_jfkd_input.npy')
y_sort = np.load(r'/Users/qinpeng/Desktop/python项目/天软/node_jfkd_target.npy') 
model = MLP()
stk_len = 31
optimizer = optim.Adam(model.parameters(),lr=0.001)
for i in range(epoch): 
    for j in range(num-1):
        data_group_in_pt = torch.tensor(data_group_in[j*batch_size:(j+1)*batch_size,:,:],dtype=torch.float32)
        y_sort_pt = torch.tensor(y_sort[j*batch_size:(j+1)*batch_size,:],dtype=torch.float32)
        model.train()
        optimizer.zero_grad()
        y_pred = model(data_group_in_pt)
        loss = distance_simple(y_pred, y_sort_pt)
        # loss.requires_grad_(True)
        loss.backward()
        optimizer.step()
        else_grad = 0
        for name,parameter in model.named_parameters():
            else_grad += parameter.grad.norm()
        with torch.no_grad():
            x_test = torch.tensor(data_group_in[225:226,:,:],dtype=torch.float32)
            out_test = encoder_plus(x_test)
            y_test = torch.tensor(y_sort[225:226,:],dtype=torch.float32)
            predict_loss = distance_simple(out_test, y_test)
            random_loss = 0
            for k in range(100):
                temp = torch.range(0, (stk_len-1))
                temp = temp[torch.randperm(temp.size(0))]
                temp = temp.unsqueeze(dim=0)
                random_loss += distance_simple(out_test, temp)
            random_loss = random_loss / 100
        # torch.set_printoptions(precision=6,  sci_mode=False)
        print(f"Epoch {i}, Step {j},Loss: {loss.item():.6f},predict_loss: {predict_loss:.6f},random_loss: {random_loss:.6f},all_grad: {else_grad:.6f}")
    
        
       
